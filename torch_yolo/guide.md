# M1000 ç”¨æˆ·æŒ‡å— - YOLO training & inference for PyTorch on MUSA

æœ¬æŒ‡å—æ—¨åœ¨å¸®åŠ©ç”¨æˆ·åœ¨ M1000 è®¾å¤‡ä¸Šé…ç½®å’Œä½¿ç”¨ PyTorchå®ŒæˆYOLO training & inferenceï¼Œä»¥å……åˆ†åˆ©ç”¨ MUSA åŠ é€Ÿèƒ½åŠ›ã€‚

## 1. å‡†å¤‡ç¯å¢ƒï¼šPyTorch on MUSA

æœ¬èŠ‚ä»‹ç»å¦‚ä½•åœ¨ M1000 ç¯å¢ƒä¸­å®‰è£… PyTorchï¼Œä»¥æ”¯æŒ MUSA åŠ é€Ÿã€‚

**å‰ææ¡ä»¶**ï¼š

*   Python ç‰ˆæœ¬ï¼š**3.10**
*   PyTorch MUSAç‰ˆæœ¬ï¼š**2.2.0**
*   numpy ç‰ˆæœ¬ï¼š**1.23.x**

**å®‰è£…æ­¥éª¤**ï¼š

æ‚¨å¯ä»¥é€‰æ‹©ä»¥ä¸‹ä¸¤ç§æ–¹å¼å®‰è£… PyTorch åŠå…¶ MUSA å…¼å®¹ç»„ä»¶ï¼š**åœ¨çº¿å®‰è£…** æˆ– **ç¦»çº¿å®‰è£…**ã€‚

### 1.1 å®‰è£…æ–¹å¼ä¸€ï¼šåœ¨çº¿å®‰è£…

å¦‚æœæ‚¨å¯ä»¥è®¿é—®äº’è”ç½‘ï¼Œæ¨èä½¿ç”¨åœ¨çº¿å®‰è£…æ–¹å¼ï¼Œå‘½ä»¤å¦‚ä¸‹ï¼š

```sh
pip install https://apollo-appstore-pre.tos-cn-beijing.volces.com/appstore/release/pip/pytorch/1.3.2/torch-2.2.0-cp310-cp310-linux_aarch64.whl
pip install https://apollo-appstore-pre.tos-cn-beijing.volces.com/appstore/release/pip/pytorch/1.3.2/torch_musa-1.3.2-cp310-cp310-linux_aarch64.whl
pip install https://apollo-appstore-pre.tos-cn-beijing.volces.com/appstore/release/pip/pytorch/1.3.2/torchaudio-2.2.2+cefdb36-cp310-cp310-linux_aarch64.whl
pip install https://apollo-appstore-pre.tos-cn-beijing.volces.com/appstore/release/pip/pytorch/1.3.2/torchvision-0.17.2+c1d70fe-cp310-cp310-linux_aarch64.whl
pip install numpy==1.23
```

AiBook 1.2.0 æœ€æ–°åŒ…ï¼š
```
torch_musaï¼š https://oss.mthreads.com/mt-ai-data/ci-release/torch_musa/AIBook/20250113/torch_musa-1.3.2-cp310-cp310-linux_aarch64.whl
pytorchï¼š    https://oss.mthreads.com/mt-ai-data/ci-release/torch_musa/AIBook/20250113/torch-2.2.0-cp310-cp310-linux_aarch64.whl
torchvisionï¼šhttps://oss.mthreads.com/mt-ai-data/ci-release/torch_musa/AIBook/20250113/torchvision-0.17.2+c1d70fe-cp310-cp310-linux_aarch64.whl
torchaudioï¼š https://oss.mthreads.com/mt-ai-data/ci-release/torch_musa/AIBook/20250113/torchaudio-2.2.2+cefdb36-cp310-cp310-linux_aarch64.whl
MTTï¼š        https://oss.mthreads.com/ai-product/aipc/mtt/mttransformer-20240402.dev67+g1196b79-py3-none-any.whl
vllmï¼š       https://oss.mthreads.com/ai-product/aipc/mtt/vllm-0.4.2+musa-cp310-cp310-linux_aarch64.whl
tritonï¼š     https://oss.mthreads.com/ai-product/aipc/triton/triton-3.0.0-cp310-cp310-linux_aarch64.whl
```

### 1.2 å®‰è£…æ–¹å¼äºŒï¼šç¦»çº¿å®‰è£…

å¦‚æœæ‚¨æ— æ³•è®¿é—®äº’è”ç½‘ï¼Œæˆ–è€…ç½‘ç»œç¯å¢ƒä¸ç¨³å®šï¼Œè¯·ä½¿ç”¨ç¦»çº¿å®‰è£…æ–¹å¼ã€‚

**æ­¥éª¤ 1ï¼šä¸‹è½½ whl æ–‡ä»¶**

é¦–å…ˆï¼Œä½¿ç”¨ `wget` å‘½ä»¤ä¸‹è½½æ‰€æœ‰å¿…è¦çš„ whl å®‰è£…åŒ…ï¼š

```sh
wget https://apollo-appstore-pre.tos-cn-beijing.volces.com/appstore/release/pip/pytorch/1.3.2/torch-2.2.0-cp310-cp310-linux_aarch64.whl
wget https://apollo-appstore-pre.tos-cn-beijing.volces.com/appstore/release/pip/pytorch/1.3.2/torch_musa-1.3.2-cp310-cp310-linux_aarch64.whl
wget https://apollo-appstore-pre.tos-cn-beijing.volces.com/appstore/release/pip/pytorch/1.3.2/torchaudio-2.2.2+cefdb36-cp310-cp310-linux_aarch64.whl
wget https://apollo-appstore-pre.tos-cn-beijing.volces.com/appstore/release/pip/pytorch/1.3.2/torchvision-0.17.2+c1d70fe-cp310-cp310-linux_aarch64.whl
```

**æ­¥éª¤ 2ï¼šç¦»çº¿å®‰è£… whl æ–‡ä»¶**

ä¸‹è½½å®Œæˆåï¼Œä½¿ç”¨ `pip install` å‘½ä»¤å®‰è£…è¿™äº› whl åŒ…ï¼š

```sh
pip install torch-2.2.0-cp310-cp310-linux_aarch64.whl
pip install torch_musa-1.3.2-cp310-cp310-linux_aarch64.whl
pip install torchaudio-2.2.2+cefdb36-cp310-cp310-linux_aarch64.whl
pip install torchvision-0.17.2+c1d70fe-cp310-cp310-linux_aarch64.whl
pip install numpy==1.23   # numpyé»˜è®¤å®‰è£…2.2ä¼šæœ‰æŠ¥é”™ï¼Œè¯·ä½¿ç”¨1.23.xç‰ˆæœ¬
```

**æ³¨æ„**ï¼š

*   è¯·åŠ¡å¿…å®‰è£…ä½¿ç”¨Numpy 1.23.xç‰ˆæœ¬ï¼Œä»¥é¿å…ä¸ PyTorch 2.2.0 ç‰ˆæœ¬å¯èƒ½å­˜åœ¨çš„å…¼å®¹æ€§é—®é¢˜ã€‚

## 2. å®‰è£…ï¼štorch å…¼å®¹åŒ…

ä¸ºäº†æ›´å¥½åœ°å…¼å®¹ç°æœ‰çš„ PyTorch ä»£ç ï¼Œæ‚¨éœ€è¦å®‰è£… `torch_compat` å…¼å®¹åŒ…ã€‚

**å®‰è£…å‘½ä»¤**ï¼š

```sh
pip install https://apollo-appstore-pre.tos-cn-beijing.volces.com/appstore/release/pip/torch_compat/torch_compat-1.0.0-cp310-cp310-linux_aarch64.whl
```

## 3. æœ€ä½³å®è·µ

æœ¬èŠ‚æä¾›äº†ä¸€äº›åœ¨ M1000 ä¸Šä½¿ç”¨ PyTorch çš„æœ€ä½³å®è·µç¤ºä¾‹ã€‚

### 3.1 åŸºç¡€ç”¨ä¾‹

ä»¥ä¸‹æ˜¯ä¸€ä¸ªåŸºç¡€çš„ PyTorch ç”¨ä¾‹ï¼Œå±•ç¤ºäº†å¦‚ä½•åœ¨ MUSA è®¾å¤‡ä¸Šè¿è¡Œå¼ é‡è¿ç®—ã€‚

**ç¤ºä¾‹ä»£ç  (test_cuda.py)**ï¼š

```py
#!/usr/bin/env python3
import torch_compat as torch        # å¯¼å…¥torch_compatåŒ…ï¼Œå¹¶é‡å‘½åä¸ºtorchï¼Œå…¶ä½™ä¿æŒä¸å˜

x = torch.randn(2, 3)

y1 = x.to("cuda:0")
print(f"y1 device: {y1.device}")

y2 = x.to(device="cuda:0")
print(f"y2 device: {y2.device}")

y3 = x.to("cpu") # Should not be changed
print(f"y3 device: {y3.device}")

y4 = x.to(0) # Assuming 0 is cuda device index, might need adjustment based on your setup if indices are different
print(f"y4 device: {y4.device}")

y5 = x.to(device=0) # Assuming 1 is cuda device index
print(f"y5 device: {y5.device}")

y6 = x.to(dtype=torch.float64, device="cuda:0") # Mixed args and kwargs
print(f"y6 device: {y6.device}, dtype: {y6.dtype}")

y7 = x.to(device="cuda:0", non_blocking=True) # More kwargs
print(f"y7 device: {y7.device}, non_blocking: {y7.is_pinned()}")

x = torch.tensor([1]).to('cuda:0')
print(x.device)

print(torch.__version__)
print(torch.cuda.is_available())

device = torch.device('cuda:0')
print(device)
```

**è¿è¡Œç»“æœ**ï¼š

æ­£å¸¸è¿è¡Œä¸Šè¿°ä»£ç ï¼Œæ‚¨å°†çœ‹åˆ°ä»¥ä¸‹è¾“å‡ºä¿¡æ¯ï¼Œè¡¨æ˜ PyTorch ä»£ç å·²åœ¨ MUSA è®¾å¤‡ (`musa:0`) ä¸ŠæˆåŠŸè¿è¡Œï¼š

```txt
$ ./test_cuda.py
y1 device: musa:0
y2 device: musa:0
y3 device: cpu
y4 device: musa:0
y5 device: musa:0
y6 device: musa:0, dtype: torch.float64
y7 device: musa:0, non_blocking: False
musa:0
2.2.0
True
musa:0
```

### 3.2 æ¡ˆä¾‹ï¼šYOLOè®­æ¨ä¸€ä½“

æœ¬èŠ‚å±•ç¤ºå¦‚ä½•å°†æµè¡Œçš„ç›®æ ‡æ£€æµ‹æ¨¡å‹ YOLO é€‚é…åˆ° MUSA ç¯å¢ƒä¸Šè¿è¡Œã€‚

#### 3.2.1 å‡†å¤‡å·¥ä½œ

åœ¨è¿è¡Œ YOLO ç¤ºä¾‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å®Œæˆä»¥ä¸‹å‡†å¤‡å·¥ä½œï¼š

**æ­¥éª¤ 1ï¼šå®‰è£… ultralytics**

ä½¿ç”¨ pip å®‰è£… YOLOv8 å®˜æ–¹åº“ `ultralytics`ï¼š

```sh
pip install ultralytics
```

**æ­¥éª¤ 2ï¼šä¸‹è½½æµ‹è¯•å›¾ç‰‡**

ä¸‹è½½ä¸€å¼ ç”¨äºæµ‹è¯•çš„å›¾ç‰‡ `bus.jpg`ï¼š

```sh
wget https://ultralytics.com/images/bus.jpg
```

#### 3.2.2 ç¤ºä¾‹ä»£ç  (yolo_train_infer.py)

ä»¥ä¸‹æ˜¯ YOLO é€‚é… MUSA çš„ç¤ºä¾‹ä»£ç ï¼Œæ‚¨å¯ä»¥ä¿å­˜ä¸º `yolo_train_infer.py` æ–‡ä»¶ï¼š

```py
#!/usr/bin/env python3
import torch_compat as torch
from ultralytics import YOLO

# Load a model
model = YOLO("yolo11n.pt")

# Train the model
train_results = model.train(
    data="coco8.yaml",  # path to dataset YAML
    epochs=100,  # number of training epochs
    imgsz=640,  # training image size
    device="0",  # device to run on, i.e. device=0 or device=0,1,2,3 or device=cpu
)

# Evaluate model performance on the validation set
metrics = model.val()

# Perform object detection on an image
results = model("./bus.jpg")
results[0].show()
```

#### 3.2.3 è¿è¡Œä¸é—®é¢˜è§£å†³

**è¿è¡Œä»£ç **ï¼š

ç›´æ¥è¿è¡Œä¸Šè¿° `yolo_train_infer.py` è„šæœ¬ï¼š

```sh
chmod +x yolo_train_infer.py
./yolo_train_infer.py
```

**é—®é¢˜æ’æŸ¥**ï¼š

åœ¨é¦–æ¬¡è¿è¡Œæ—¶ï¼Œæ‚¨å¯èƒ½ä¼šé‡åˆ°ä¸€äº›æŠ¥é”™ã€‚ä»¥ä¸‹åˆ—å‡ºäº†ä¸€äº›å¸¸è§é—®é¢˜åŠå…¶è§£å†³æ–¹æ¡ˆï¼š

*   **[é—®é¢˜ 1: Numpy ç‰ˆæœ¬å†²çª](#4.1)**:  `NumPy 2.2.0 as it may crash. To support both 1.x and 2.x`
*   **[é—®é¢˜ 2: isinstance å‡½æ•°æŠ¥é”™](#4.2)**: `TypeError: isinstance() arg 2 must be a type, a tuple of types, or a union`

#### 3.2.4 æˆåŠŸè¿è¡Œç»“æœ

è§£å†³ä¸Šè¿°é—®é¢˜åï¼Œå†æ¬¡è¿è¡Œ `yolo_demo.py`ï¼Œæ‚¨åº”è¯¥èƒ½çœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹çš„è¿è¡Œç»“æœï¼Œè¿™è¡¨æ˜ YOLO æ¨¡å‹å·²æˆåŠŸåœ¨ MUSA ä¸Šè¿è¡Œï¼Œå¹¶å®Œæˆäº†è®­ç»ƒã€éªŒè¯å’Œæ¨ç†è¿‡ç¨‹ï¼š

```txt
      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
     98/100      1.18G     0.5199     0.5667      1.002         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.04it/s]
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.84it/s
                   all          4         17      0.839      0.464      0.529       0.28

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
     99/100      1.18G     0.5653     0.4987      1.011         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.17it/s]
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.87it/s
                   all          4         17      0.839      0.464      0.529       0.28

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
    100/100      1.18G     0.4481     0.4065     0.9326         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.81it/s]
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.85it/s
                   all          4         17      0.856      0.461      0.556      0.321

100 epochs completed in 0.042 hours.
WARNING âš ï¸ Skipping runs/detect/train10/weights/last.pt, not a valid Ultralytics model: isinstance() arg 2 must be a type, a tuple of types, or a union
WARNING âš ï¸ Skipping runs/detect/train10/weights/best.pt, not a valid Ultralytics model: isinstance() arg 2 must be a type, a tuple of types, or a union

Validating runs/detect/train10/weights/best.pt...
Ultralytics 8.3.85 ğŸš€ Python-3.10.16 torch-2.2.0 CUDA:0 (M1000, 31795MiB)
YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.58it/s
                   all          4         17       0.65      0.783      0.913      0.652
                person          3         10      0.624        0.7       0.67      0.328
                   dog          1          1      0.527          1      0.995      0.796
                 horse          1          2      0.644          1      0.995      0.676
              elephant          1          2      0.551          1      0.828      0.322
              umbrella          1          1      0.552          1      0.995      0.895
          potted plant          1          1          1          0      0.995      0.895
Speed: 0.4ms preprocess, 18.3ms inference, 0.0ms loss, 3.2ms postprocess per image
Results saved to runs/detect/train10
Ultralytics 8.3.85 ğŸš€ Python-3.10.16 torch-2.2.0 CUDA:0 (M1000, 31795MiB)
YOLO11n summary (fused): 100 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs
val: Scanning /data/repo/musa-yolov5/datasets/coco8/labels/val.cache... 4 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:0
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.16it/s
                   all          4         17      0.649      0.783      0.913      0.653
                person          3         10      0.623        0.7      0.671       0.33
                   dog          1          1      0.527          1      0.995      0.796
                 horse          1          2      0.643          1      0.995      0.676
              elephant          1          2      0.551          1      0.828      0.323
              umbrella          1          1      0.551          1      0.995      0.895
          potted plant          1          1          1          0      0.995      0.895
Speed: 0.9ms preprocess, 16.2ms inference, 0.0ms loss, 4.2ms postprocess per image
Results saved to runs/detect/train102

image 1/1 /data/tmp/pip/torch_compat/bus.jpg: 640x480 4 persons, 1 bus, 33.4ms
Speed: 5.7ms preprocess, 33.4ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 480)
```

### 3.3 æ¡ˆä¾‹ï¼šYOLO on GPUæ¨ç†æ€§èƒ½æµ‹è¯•

æœ¬èŠ‚å±•ç¤ºå¦‚ä½•è¯„ä¼° YOLO on GPUæ¨ç†æ€§èƒ½

#### 3.3.1 å‡†å¤‡å·¥ä½œ

åœ¨è¿è¡Œ YOLO ç¤ºä¾‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å®Œæˆä»¥ä¸‹å‡†å¤‡å·¥ä½œï¼š

**æ­¥éª¤ 1ï¼šå®‰è£… ultralytics**

ä½¿ç”¨ pip å®‰è£… YOLOv8 å®˜æ–¹åº“ `ultralytics`ï¼š

```sh
pip install ultralytics
```

#### 3.3.2 ç¤ºä¾‹ä»£ç  (yolo_infer_perf.py)

ä»¥ä¸‹æ˜¯ YOLO é€‚é… MUSA çš„ç¤ºä¾‹ä»£ç ï¼Œæ‚¨å¯ä»¥ä¿å­˜ä¸º `yolo_infer_perf.py` æ–‡ä»¶ï¼š

```py
#!/usr/bin/env python3

import torch_compat as torch;
from ultralytics import YOLO
from ultralytics.cfg import TASK2DATA, TASK2METRIC
from ultralytics.engine.exporter import export_formats
from ultralytics.utils import ASSETS, LOGGER
from ultralytics.utils.checks import check_imgsz, check_yolo
from ultralytics.utils.files import file_size
from ultralytics.utils.torch_utils import select_device

import argparse
import time
from itertools import product
from typing import List
from pathlib import Path

DEFAULT_MODELS = [
    "yolov5n.pt",
    "yolov5s.pt",
    "yolov5m.pt",
    "yolov5l.pt",

    "yolov8n.pt",
    "yolov8s.pt",
    "yolov8m.pt",
    "yolov8l.pt",

    "yolov10n.pt",
    "yolov10s.pt",
    "yolov10m.pt",
    "yolov10l.pt",

    "yolo11n.pt",
    "yolo11s.pt",
    "yolo11m.pt",
    "yolo11l.pt",

    "yolo12n.pt",
    "yolo12s.pt",
    "yolo12m.pt",
    "yolo12l.pt",
    ]
DEFAULT_BATCHES = [1, 2, 4, 8, 16, 32]
DEFAULT_DTYPES = [False, True]  # False: fp32, True: fp16

def benchmark(
    model,
    data=None,
    imgsz=160,
    batch=1,
    half=False,
    int8=False,
    device="cpu",
    verbose=False,
    eps=1e-3,
    format="-",
):
    imgsz = check_imgsz(imgsz)
    assert imgsz[0] == imgsz[1] if isinstance(imgsz, list) else True, "benchmark() only supports square imgsz."

    import pandas as pd  # scope for faster 'import ultralytics'

    pd.options.display.max_columns = 10
    pd.options.display.width = 120
    device = select_device(device, verbose=False)
    if isinstance(model, (str, Path)):
        model = YOLO(model)
    is_end2end = getattr(model.model.model[-1], "end2end", False)
    data = data or TASK2DATA[model.task]  # task to dataset, i.e. coco8.yaml for task=detect
    key = TASK2METRIC[model.task]  # task to metric, i.e. metrics/mAP50-95(B) for task=detect

    y = []
    t0 = time.time()

    format_arg = format.lower()
    if format_arg:
        formats = frozenset(export_formats()["Argument"])
        assert format in formats, f"Expected format to be one of {formats}, but got '{format_arg}'."
    for i, (name, format, suffix, cpu, gpu, _) in enumerate(zip(*export_formats().values())):
        emoji, filename = "âŒ", None  # export defaults
        try:
            if format_arg and format_arg != format:
                continue

            # Export
            if format == "-":
                filename = model.pt_path or model.ckpt_path or model.model_name
                exported_model = model  # PyTorch format
            else:
                filename = model.export(
                    imgsz=imgsz, format=format, half=half, int8=int8, data=data, device=device, verbose=False
                )
                exported_model = YOLO(filename, task=model.task)
                assert suffix in str(filename), "export failed"
            emoji = "â"  # indicates export succeeded

            # Predict
            assert model.task != "pose" or i != 7, "GraphDef Pose inference is not supported"
            assert i not in {9, 10}, "inference not supported"  # Edge TPU and TF.js are unsupported
            assert i != 5 or platform.system() == "Darwin", "inference only supported on macOS>=10.13"  # CoreML
            if i in {13}:
                assert not is_end2end, "End-to-end torch.topk operation is not supported for NCNN prediction yet"
            exported_model.predict(ASSETS / "bus.jpg", imgsz=imgsz, device=device, half=half, verbose=False)

            # Validate
            results = exported_model.val(
                data=data, batch=batch, imgsz=imgsz, plots=False, device=device, half=half, int8=int8, verbose=False
            )
            metric, speed = results.results_dict[key], results.speed["inference"]
            fps = round(1000 / (speed + eps), 2)  # frames per second
            y.append([name, "âœ…", round(file_size(filename), 1), round(metric, 4), round(speed, 2), fps])
        except Exception as e:
            if verbose:
                assert type(e) is AssertionError, f"Benchmark failure for {name}: {e}"
            LOGGER.warning(f"ERROR âŒï¸ Benchmark failure for {name}: {e}")
            y.append([name, emoji, round(file_size(filename), 1), None, None, None])  # mAP, t_inference

    # Print results
    check_yolo(device=device)  # print system info
    df = pd.DataFrame(y, columns=["Format", "Statusâ”", "Size (MB)", key, "Inference time (ms/im)", "FPS"])

    name = model.model_name
    dt = time.time() - t0
    legend = "Benchmarks legend:  - âœ… Success  - â Export passed but validation failed  - âŒï¸ Export failed"
    s = f"\nBenchmarks complete for {name} on {data} at imgsz={imgsz} ({dt:.2f}s)\n{legend}\n{df.fillna('-')}\n"
    LOGGER.info(s)
    with open("benchmarks.log", "a", errors="ignore", encoding="utf-8") as f:
        f.write(s)

    if verbose and isinstance(verbose, float):
        metrics = df[key].array  # values to compare to floor
        floor = verbose  # minimum metric floor to pass, i.e. = 0.29 mAP for YOLOv5n
        assert all(x > floor for x in metrics if pd.notna(x)), f"Benchmark failure: metric(s) < floor {floor}"

    return df

def main(models: List[str], batches: List[int], dtypes: List[bool], dataset: str = "coco128.yaml", imgsz: int = 640, device: str = "cuda:0"):
    for half, model, batch in product(dtypes, models, batches):
        print(f"Benchmarking model: {model} with half:{half} batch:{batch} dataset:{dataset} imgsz:{imgsz}")
        benchmark(model=model, data=dataset, imgsz=imgsz, batch=batch, half=half, int8=True, device=device)
        time.sleep(5)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run YOLO benchmarks.")
    parser.add_argument("--models", nargs="+", default=DEFAULT_MODELS, help="List of models to benchmark.")
    parser.add_argument("--batches", nargs="+", type=int, default=DEFAULT_BATCHES, help="List of batch to test.")
    parser.add_argument("--dtypes", nargs="+", type=lambda x: x.lower() == 'true', default=DEFAULT_DTYPES, help="List of dtypes to test (False for fp32, True for fp16).")
    parser.add_argument("--dataset", default="coco128.yaml", help="Dataset configuration file (e.g., coco128.yaml).")
    parser.add_argument("--imgsz", type=int, default=640, help="Image size.")
    parser.add_argument("--device", default="cuda:0", help="Device to run on.")

    args = parser.parse_args()

    main(args.models, args.batches, args.dtypes, args.dataset, args.imgsz, args.device)
```

#### 3.3.3 è¿è¡Œä¸é—®é¢˜è§£å†³

**è¿è¡Œä»£ç **ï¼š

ç›´æ¥è¿è¡Œä¸Šè¿° `yolo_infer_perf.py` è„šæœ¬ï¼š

```sh
chmod +x yolo_infer_perf.py
./yolo_infer_perf.py
```

**é—®é¢˜æ’æŸ¥**ï¼š

åœ¨é¦–æ¬¡è¿è¡Œæ—¶ï¼Œæ‚¨å¯èƒ½ä¼šé‡åˆ°ä¸€äº›æŠ¥é”™ã€‚ä»¥ä¸‹åˆ—å‡ºäº†ä¸€äº›å¸¸è§é—®é¢˜åŠå…¶è§£å†³æ–¹æ¡ˆï¼š

*   **[é—®é¢˜ 1: Numpy ç‰ˆæœ¬å†²çª](#4.1)**:  `NumPy 2.2.0 as it may crash. To support both 1.x and 2.x`
*   **[é—®é¢˜ 2: isinstance å‡½æ•°æŠ¥é”™](#4.2)**: `TypeError: isinstance() arg 2 must be a type, a tuple of types, or a union`

#### 3.3.4 æˆåŠŸè¿è¡Œç»“æœ

è§£å†³ä¸Šè¿°é—®é¢˜åï¼Œå†æ¬¡è¿è¡Œ `yolo_demo.py`ï¼Œæ‚¨åº”è¯¥èƒ½çœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹çš„è¿è¡Œç»“æœï¼Œè¿™è¡¨æ˜ YOLO æ¨¡å‹å·²æˆåŠŸåœ¨ MUSA ä¸Šè¿è¡Œï¼Œå¹¶å®Œæˆäº†è®­ç»ƒã€éªŒè¯å’Œæ¨ç†è¿‡ç¨‹ï¼š

```txt
$ ./yolo_infer_perf.py
Benchmarking model: yolo12n.pt with half:False batch:1 dataset:coco128.yaml imgsz:640
Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo12n.pt to 'yolo12n.pt'...
  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                                            | 448k/5.34M [00:00<00:03,  12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                                      | 672k/5.34M [00:00<00:04,  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                                           | 1.02M/5.34M [00:00<00:02,  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                                                               | 1.44M/5.34M [00:00<00:01,  32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                       | 1.70M/5.34M [00:00<00:01,  36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                | 1.95M/5.34M [00:01<00:01,  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                          | 2.18M/5.34M [00:01<00:01,  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                   | 2.41M/5.34M [00:01<00:01,  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                            | 2.64M/5.34M [00:01<00:01,  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                      | 2.86M/5.34M [00:01<00:01,  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                               | 3.11M/5.34M [00:01<00:01,  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                         | 3.32M/5.34M [00:01<00:00,  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                  | 3.56M/5.34M [00:01<00:00,  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                            | 3.78M/5.34M [00:01<00:00,  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                      | 3.99M/5.34M [00:02<00:00,  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 4.45M/5.34M [00:02<00:00,  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                  | 4.70M/5.34M [00:02<00:00,  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹           | 4.94M/5.34M [00:02<00:00,  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 5.16M/5.34M [00:02<00:00, 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.34M/5.34M [00:02<00:00, 1.98MB/s]
val: Scanning /data/repo/musa-yolov5/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<?, ?it/s]
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:06<00:00, 19.14it/s]
                   all        128        929      0.692      0.616      0.683      0.528
Speed: 0.7ms preprocess, 30.8ms inference, 0.0ms loss, 4.0ms postprocess per image
Setup complete âœ… (12 CPUs, 31.0 GB RAM, 60.3/125.2 GB disk)

Benchmarks complete for yolo12n.pt on coco128.yaml at imgsz=640 (10.60s)
Benchmarks legend:  - âœ… Success  - â Export passed but validation failed  - âŒï¸ Export failed
    Format Statusâ”  Size (MB)  metrics/mAP50-95(B)  Inference time (ms/im)    FPS
0  PyTorch       âœ…        5.3               0.5277                   30.84  32.43

Benchmarking model: yolo12n.pt with half:False batch:2 dataset:coco128.yaml imgsz:640
val: Scanning /data/repo/musa-yolov5/datasets/coco128/labels/train2017.cache... 126 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:00<?, ?it/s]
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:04<00:00, 15.98it/s]
                   all        128        929      0.697      0.619      0.684      0.529
Speed: 0.5ms preprocess, 19.1ms inference, 0.0ms loss, 5.7ms postprocess per image
Setup complete âœ… (12 CPUs, 31.0 GB RAM, 60.3/125.2 GB disk)

Benchmarks complete for yolo12n.pt on coco128.yaml at imgsz=640 (4.88s)
Benchmarks legend:  - âœ… Success  - â Export passed but validation failed  - âŒï¸ Export failed
    Format Statusâ”  Size (MB)  metrics/mAP50-95(B)  Inference time (ms/im)    FPS
0  PyTorch       âœ…        5.3               0.5285                   19.06  52.46
```

## 4. å¸¸è§é—®é¢˜ FAQ

æœ¬èŠ‚æ±‡æ€»äº†ç”¨æˆ·åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­å¯èƒ½é‡åˆ°çš„å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆï¼Œä»¥ä¾¿å¿«é€Ÿæ’æŸ¥å’Œè§£å†³é—®é¢˜ã€‚

### <span id="4.1">4.1 Numpy ç‰ˆæœ¬å†²çª</span>ï¼šNumPy 2.2.0 å¯èƒ½å¯¼è‡´ç¨‹åºå´©æºƒ

**é—®é¢˜æè¿°**ï¼š

è¿è¡Œç¨‹åºæ—¶ï¼Œå‡ºç°ç±»ä¼¼ä»¥ä¸‹é”™è¯¯ä¿¡æ¯ï¼Œæç¤º NumPy ç‰ˆæœ¬ 2.2.0 å¯èƒ½å¯¼è‡´ç¨‹åºå´©æºƒï¼Œå»ºè®®é™çº§åˆ° NumPy 1.x ç‰ˆæœ¬ã€‚

```txt
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/data/tmp/pip/torch_compat/./yolo_demo.py", line 3, in <module>
    import torch_compat as torch
  ... (çœç•¥éƒ¨åˆ† traceback) ...
/home/mt/miniconda3/envs/ultralytics/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy:
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

å®‰è£…æŒ‡å®šçš„ `numpy==1.23.5` ç‰ˆæœ¬ã€‚

```sh
pip install numpy==1.23.5       # å®‰è£…numpy 1.23.x ç‰ˆæœ¬
```

### <span id="4.2">4.2 `isinstance` å‡½æ•°æŠ¥é”™</span>ï¼šTypeError: isinstance() arg 2 must be a type, a tuple of types, or a union

**é—®é¢˜æè¿°**ï¼š

è¿è¡Œ YOLO ç­‰æ¨¡å‹æ—¶ï¼Œå‡ºç° `TypeError: isinstance() arg 2 must be a type, a tuple of types, or a union` é”™è¯¯ï¼ŒæŒ‡å‘ `ultralytics/utils/torch_utils.py` æ–‡ä»¶ã€‚

```txt
$ ./yolo_demo.py
Traceback (most recent call last):
  File "/data/tmp/pip/torch_compat/./yolo_demo.py", line 10, in <module>
    train_results = model.train(
  ... (çœç•¥éƒ¨åˆ† traceback) ...
  File "/home/mt/miniconda3/envs/ultralytics/lib/python3.10/site-packages/ultralytics/utils/torch_utils.py", line 166, in select_device
    if isinstance(device, torch.device) or str(device).startswith("tpu"):
TypeError: isinstance() arg 2 must be a type, a tuple of types, or a union
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

æ ¹æ®é”™è¯¯æç¤ºï¼Œå®šä½åˆ°æŠ¥é”™æ–‡ä»¶ `ultralytics/utils/torch_utils.py` å’ŒæŠ¥é”™è¡Œå·ï¼ˆä¾‹å¦‚ç¬¬ 166 è¡Œï¼‰ã€‚

1.  **æ‰“å¼€æŠ¥é”™æ–‡ä»¶**ï¼šä½¿ç”¨ç¼–è¾‘å™¨æ‰“å¼€ `ultralytics/utils/torch_utils.py` æ–‡ä»¶ã€‚
2.  **å®šä½æŠ¥é”™ä»£ç **ï¼šæ‰¾åˆ°æŠ¥é”™è¡Œï¼Œé€šå¸¸æ˜¯ç±»ä¼¼ `isinstance(device, torch.device)` çš„ä»£ç ã€‚
3.  **ä¿®æ”¹æºç **ï¼šå°† `isinstance(device, torch.device)` ä¿®æ”¹ä¸º `isinstance(device, torch._C.device)`ã€‚

**ä¿®æ”¹ç¤ºä¾‹**ï¼š

å°†ä»¥ä¸‹ä»£ç ï¼š

```python
if isinstance(device, torch.device) or str(device).startswith("tpu"):
```

ä¿®æ”¹ä¸ºï¼š

```python
if isinstance(device, torch._C.device) or str(device).startswith("tpu"):
```

**å®Œæˆä¿®æ”¹åï¼Œé‡æ–°è¿è¡Œç¨‹åºå³å¯è§£å†³è¯¥é—®é¢˜ã€‚**

---
